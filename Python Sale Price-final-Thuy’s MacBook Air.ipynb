{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef1de9d",
   "metadata": {},
   "source": [
    "# 1. Business Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8aa94c",
   "metadata": {},
   "source": [
    "This notebook is for the Kaggle competition hosted by Aston University. In this competition, I came in first place. The competition's results can be found here: https://www.kaggle.com/competitions/bnm861-2022/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be6db0",
   "metadata": {},
   "source": [
    "# 2. Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "\n",
    "# base libraries\n",
    "import os\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#statistics package\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Pre-processing and transformation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from functools import reduce\n",
    "\n",
    "# do not print warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed31adc",
   "metadata": {},
   "source": [
    "# 3. Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b73e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the training and testing set\n",
    "train = pd.read_csv(\"CW_training_final.csv\")\n",
    "test = pd.read_csv(\"CW_testing_kaggle.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the 'id' column\n",
    "train_ID = train['id']\n",
    "test_ID = test['id']\n",
    "\n",
    "#drop id\n",
    "train.drop(\"id\", axis = 1, inplace = True)\n",
    "test.drop(\"id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7e457",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988adee",
   "metadata": {},
   "source": [
    "## 4.1 Numerical Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of numerical variables \n",
    "numerical_vis = train.drop(columns=['YearBuilt','MoSold',\n",
    "                                   'YrSold','SalePrice','Fireplaces','YearRemodAdd'])\n",
    "numerical_vis.hist(bins=60, figsize=(30,30), histtype='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2fd34c",
   "metadata": {},
   "source": [
    "## 4.2 Categorical Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfde1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of categorical variables \n",
    "\n",
    "categorical_features = [\"MSZoning\", \"Street\",\"Alley\",\"LotShape\",\"LandContour\",\"Utilities\",\"LotConfig\",\"LandSlope\"\n",
    "                       ]\n",
    "fig, ax = plt.subplots(1, len(categorical_features))\n",
    "for i, categorical_feature in enumerate(train[categorical_features]):\n",
    "    train[categorical_feature].value_counts().plot(kind='bar', figsize=(25, 4),ax=ax[i]).set_title(categorical_feature)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"Exterior2nd\", \"Condition1\",\"Condition2\",\"BldgType\",\"HouseStyle\",\"RoofStyle\",\"RoofMatl\",\"Exterior1st\"\n",
    "                       ]\n",
    "fig, ax = plt.subplots(1, len(categorical_features))\n",
    "for i, categorical_feature in enumerate(train[categorical_features]):\n",
    "    train[categorical_feature].value_counts().plot(kind='bar', figsize=(25, 4),ax=ax[i]).set_title(categorical_feature)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73192ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(\"Neighborhood\").size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"MasVnrType\", \"ExterQual\",\"ExterCond\",\"Foundation\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\"\n",
    "                       ]\n",
    "fig, ax = plt.subplots(1, len(categorical_features))\n",
    "for i, categorical_feature in enumerate(train[categorical_features]):\n",
    "    train[categorical_feature].value_counts().plot(kind='bar', figsize=(25, 4),ax=ax[i]).set_title(categorical_feature)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b19867",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"BsmtFinType2\", \"Heating\",\"HeatingQC\",\"CentralAir\",\"Electrical\",\"KitchenQual\",\"Functional\",\"FireplaceQu\"\n",
    "                       ]\n",
    "fig, ax = plt.subplots(1, len(categorical_features))\n",
    "for i, categorical_feature in enumerate(train[categorical_features]):\n",
    "    train[categorical_feature].value_counts().plot(kind='bar', figsize=(25, 4),ax=ax[i]).set_title(categorical_feature)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"GarageType\", \"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PavedDrive\",\"PoolQC\",\"Fence\",\"MiscFeature\",\n",
    "                        \"SaleType\", \"SaleCondition\"]\n",
    "fig, ax = plt.subplots(1, len(categorical_features))\n",
    "for i, categorical_feature in enumerate(train[categorical_features]):\n",
    "    train[categorical_feature].value_counts().plot(kind='bar', figsize=(25, 4),ax=ax[i]).set_title(categorical_feature)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80900972",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade0c309",
   "metadata": {},
   "source": [
    "# 5. Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb028d8",
   "metadata": {},
   "source": [
    "## 5.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add features on trainset\n",
    "train['Age Sold'] = train['YrSold'] - train['YearBuilt'] \n",
    "train['Age Sold Renovation'] = train['YrSold'] - train['YearRemodAdd'] \n",
    "train['Age Sold Garage'] = train['YrSold'] - train['GarageYrBlt']\n",
    "train[\"SqFtPerRoom\"] = train[\"GrLivArea\"] / (train[\"TotRmsAbvGrd\"] +\n",
    "                                                       train[\"FullBath\"] +\n",
    "                                                       train[\"HalfBath\"] +\n",
    "                                                       train[\"KitchenAbvGr\"])\n",
    "train['Total_Bathrooms'] = train['FullBath'] +  train['HalfBath'] + train['BsmtFullBath'] + train['BsmtHalfBath']\n",
    "train['Total Area'] = train['TotalBsmtSF'] +  train['1stFlrSF'] + train['2ndFlrSF'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a032d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add features on trainset\n",
    "test['Age Sold'] = test['YrSold'] - test['YearBuilt'] \n",
    "test['Age Sold Renovation'] = test['YrSold'] - test['YearRemodAdd'] \n",
    "test['Age Sold Garage'] = test['YrSold'] - test['GarageYrBlt']\n",
    "test[\"SqFtPerRoom\"] = test[\"GrLivArea\"] / (test[\"TotRmsAbvGrd\"] +\n",
    "                                                       test[\"FullBath\"] +\n",
    "                                                       test[\"HalfBath\"] +\n",
    "                                                       test[\"KitchenAbvGr\"])\n",
    "test['Total_Bathrooms'] = test['FullBath'] +  test['HalfBath'] + test['BsmtFullBath'] + test['BsmtHalfBath']\n",
    "test['Total Area'] = test['TotalBsmtSF'] +  test['1stFlrSF'] + test['2ndFlrSF'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a523dd88",
   "metadata": {},
   "source": [
    "## 5.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff1efb",
   "metadata": {},
   "source": [
    "### 5.2.1 Numerical Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b290f0",
   "metadata": {},
   "source": [
    "Since Lot Frontage has a lot of missing values, I'll build a regression model to predict the missing values in Lot Frontage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8715051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a supervised model to predict the missing values\n",
    "#catboost regressor to predict lotfrontage\n",
    "\n",
    "#only use numerical variables to predict\n",
    "train1 = train.drop('SalePrice',1)\n",
    "train1 = train1.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "\n",
    "#set null values of lot frontage in trainset as small testset\n",
    "test_lotfrontage = train1[train1['LotFrontage'].isnull()]\n",
    "\n",
    "#drop all null values in train1\n",
    "train1.dropna(inplace = True)\n",
    "\n",
    "#take the available values of lot frontage and set them as the y_train\n",
    "y_train_lotfrontage =train1['LotFrontage']\n",
    "\n",
    "#the x_train will be all columns in train1 except 'Lot Frontage'\n",
    "x_train_lotfrontage=train1.drop('LotFrontage', axis =1)\n",
    "\n",
    "# the x_test for lot frontage will be all columns in test_lotfrontage except 'Lot Frontage'\n",
    "x_test_lotfrontage= test_lotfrontage.drop('LotFrontage', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model = CatBoostRegressor()\n",
    "\n",
    "parameters = {'depth' : [6,8,10],\n",
    "              'learning_rate' : [0.01, 0.05, 0.1],\n",
    "              'iterations'    : [30, 50, 80]\n",
    "              }\n",
    "\n",
    "grid = make_pipeline(RobustScaler(),TuneGridSearchCV(estimator=model, param_grid = parameters, cv = 10, n_jobs=-1,refit=True))\n",
    "grid.fit(x_train_lotfrontage, y_train_lotfrontage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe1587",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_pred_train = grid.predict(x_test_lotfrontage)\n",
    "train.loc[train.LotFrontage.isnull(),'LotFrontage']=lf_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e74ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat the same prediction on testset for lot frontage\n",
    "\n",
    "# use a supervised model to predict the missing values\n",
    "#catboost regressor to predict lotfrontage\n",
    "\n",
    "#only use numerical variables to predict\n",
    "test1 = test.select_dtypes(include=['float64','int64'])\n",
    "\n",
    "#set null values of lot frontage in trainset as small testset\n",
    "test_lotfrontage_te = test1[test1['LotFrontage'].isnull()]\n",
    "\n",
    "#drop all null values in train1\n",
    "test1.dropna(inplace = True)\n",
    "\n",
    "#take the available values of lot frontage and set them as the y_train\n",
    "y_train_lotfrontage_te =test1['LotFrontage']\n",
    "\n",
    "#the x_train will be all columns in train1 except 'Lot Frontage'\n",
    "x_train_lotfrontage_te =test1.drop('LotFrontage', axis =1)\n",
    "\n",
    "# the x_test for lot frontage will be all columns in test_lotfrontage except 'Lot Frontage'\n",
    "x_test_lotfrontage_te = test_lotfrontage_te.drop('LotFrontage', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f06d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model = CatBoostRegressor()\n",
    "\n",
    "parameters = {'depth' : [6,8,10],\n",
    "              'learning_rate' : [0.01, 0.05, 0.1],\n",
    "              'iterations'    : [30, 50, 80]\n",
    "              }\n",
    "\n",
    "grid = make_pipeline(RobustScaler(),TuneGridSearchCV(estimator=model, param_grid = parameters, cv = 10, n_jobs=-1,refit=True))\n",
    "grid.fit(x_train_lotfrontage_te, y_train_lotfrontage_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f486cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_pred_test = grid.predict(x_test_lotfrontage_te)\n",
    "test.loc[test.LotFrontage.isnull(),'LotFrontage']=lf_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f412ca",
   "metadata": {},
   "source": [
    "Since other numerical variables do not have to many missing values, I'll impute median values for all the NAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop labels for training set, but keep all others\n",
    "Xtrain = train.drop(\"SalePrice\", axis=1)\n",
    "Xtest = test\n",
    "\n",
    "\n",
    "ytrain = train[\"SalePrice\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeafad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the SimpleImputer class and instantiate the object\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# instantiate imputer object with median\n",
    "imputer = SimpleImputer(strategy ='median')\n",
    "\n",
    "def imputefunc(Xtrain, Xtest):\n",
    "    \n",
    "    # seperate features into numerical and categorical sets\n",
    "    x_train_num = Xtrain.select_dtypes(include=['float64','int64'])\n",
    "    x_train_cat = Xtrain.select_dtypes(include=['object'])\n",
    "\n",
    "    x_test_num = Xtest.select_dtypes(include=['float64','int64'])\n",
    "    x_test_cat = Xtest.select_dtypes(include=['object'])\n",
    "\n",
    "    # fit the impute on our training set and subsequently transform both sets\n",
    "    Xtrain = pd.DataFrame(imputer.fit_transform(x_train_num),columns = x_train_num.columns)\n",
    "    Xtest = pd.DataFrame(imputer.transform(x_test_num), columns = x_test_num.columns)\n",
    "    \n",
    "    # reset the index of both our sets as concatenation requires consistent indexes\n",
    "    x_train_cat.reset_index(level=0, inplace=True)\n",
    "    x_test_cat.reset_index(level=0, inplace=True)  \n",
    "\n",
    "    # drop the old indexes of the train and test set categoricals\n",
    "    x_train_cat.drop(columns='index', axis=1, inplace=True)\n",
    "    x_test_cat.drop(columns='index', axis=1, inplace=True)\n",
    "\n",
    "    # next we join back the two dataframes \n",
    "    Xtrain = pd.concat([Xtrain, x_train_cat], axis=1)\n",
    "    Xtest = pd.concat([Xtest, x_test_cat], axis=1)\n",
    "\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "\n",
    "Xtrain, Xtest = imputefunc(Xtrain, Xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c9bd0",
   "metadata": {},
   "source": [
    "In this dataset, for the categorical variables, some houses does not have the features so the values were left blank. I will fill the NAN with the right string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbc9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the NA with string value for the trainset\n",
    "Xtrain['PoolQC'].fillna(\"No Pool\", inplace=True)\n",
    "Xtrain['Alley'].fillna(\"No Alley Access\", inplace=True)\n",
    "Xtrain['FireplaceQu'].fillna(\"No Fireplace\", inplace=True)\n",
    "Xtrain['Fence'].fillna(\"No Fence\", inplace=True)\n",
    "Xtrain['MiscFeature'].fillna(\"No MiscFeature\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the NA with string value for the testset\n",
    "Xtest['PoolQC'].fillna(\"No Pool\", inplace=True)\n",
    "Xtest['Alley'].fillna(\"No Alley Access\", inplace=True)\n",
    "Xtest['FireplaceQu'].fillna(\"No Fireplace\", inplace=True)\n",
    "Xtest['Fence'].fillna(\"No Fence\", inplace=True)\n",
    "Xtest['MiscFeature'].fillna(\"No MiscFeature\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28971f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The rest will be imputed with mode value of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05097c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the SimpleImputer class and instantiate the object\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# instantiate imputer object with median\n",
    "imputer = SimpleImputer(strategy ='most_frequent')\n",
    "\n",
    "# define a function that imputes missing values on a given train,test dataset pair.\n",
    "def imputefunc(Xtrain, Xtest):\n",
    "    \n",
    "    # seperate features into numerical and categorical sets\n",
    "    Xtrain_num = Xtrain.select_dtypes(include=['float64','int64'])\n",
    "    Xtrain_cat = Xtrain.select_dtypes(include=['object'])\n",
    "\n",
    "    Xtest_num = Xtest.select_dtypes(include=['float64','int64'])\n",
    "    Xtest_cat = Xtest.select_dtypes(include=['object'])\n",
    "\n",
    "    # fit the impute on our training set and subsequently transform both sets\n",
    "    Xtrain = pd.DataFrame(imputer.fit_transform(Xtrain_cat),columns = Xtrain_cat.columns)\n",
    "    Xtest = pd.DataFrame(imputer.transform(Xtest_cat), columns = Xtest_cat.columns)\n",
    "    \n",
    "    # reset the index of both our sets as concatenation requires consistent indexes\n",
    "    Xtrain_num.reset_index(level=0, inplace=True)\n",
    "    Xtest_num.reset_index(level=0, inplace=True)  \n",
    "\n",
    "    # drop the old indexes of the train and test set categoricals\n",
    "    Xtrain_num.drop(columns='index', axis=1, inplace=True)\n",
    "    Xtest_num.drop(columns='index', axis=1, inplace=True)\n",
    "\n",
    "    # next we join back the two dataframes \n",
    "    Xtrain = pd.concat([Xtrain, Xtrain_num], axis=1)\n",
    "    Xtest = pd.concat([Xtest, Xtest_num], axis=1)\n",
    "\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "\n",
    "Xtrain, Xtest = imputefunc(Xtrain, Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098d8ff",
   "metadata": {},
   "source": [
    "## 5.3 Encoding Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52386bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the onehotencoder class to implement encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# set aside the categorical columns as a list object \n",
    "catcols = Xtrain.select_dtypes(['object']).columns.tolist()\n",
    "\n",
    "# define the get dummies function to return encoded train and test sets\n",
    "def get_dummies(Xtrain, Xtest, old_col_name):\n",
    "    \"\"\"Given a trainset, a testset, and the name of a column holding a \n",
    "    categorical variable, fit an encoder on the trainset, and use the \n",
    "    encoder to add dummy columns into the trainset and testset\n",
    "    \"\"\"\n",
    "    \n",
    "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    \n",
    "    # the input to the encoder must be a 2-d numpy array,\n",
    "    # so we take the column, extract their values and reshape the array to be 2-d\n",
    "    # the old_col_name will be taken from the categorical columns list defined previously\n",
    "    cat_vals = Xtrain[old_col_name].values.reshape(-1,1)\n",
    "\n",
    "    # fit an encoder and transform the **trainset**\n",
    "    # the output is a new numpy array\n",
    "    transformed = one_hot_encoder.fit_transform(cat_vals)\n",
    "    \n",
    "    # in the list of new columns, convert numpy array to a list and\n",
    "    # drop the first column, because we requested \"drop='first'\"\n",
    "    new_col_names = one_hot_encoder.categories_[0].tolist()[1:]\n",
    "    \n",
    "    for i, new_col_name in enumerate(new_col_names):\n",
    "        \n",
    "        # put the transformed data as columns in the trainset dataframe\n",
    "        Xtrain[new_col_name] = transformed[:,i]\n",
    "    \n",
    "    # transform the **testset** using the fitted encoder\n",
    "    \n",
    "    cat_vals = Xtest[old_col_name].values.reshape(-1,1)\n",
    "    transformed = one_hot_encoder.transform(cat_vals)\n",
    "\n",
    "    for i, new_col_name in enumerate(new_col_names):\n",
    "        \n",
    "        # put the transformed data as columns in the testset dataframe\n",
    "        Xtest[new_col_name] = transformed[:,i]\n",
    "    \n",
    "    return Xtrain, Xtest\n",
    "\n",
    "\n",
    "for col_name in catcols:\n",
    "    Xtrain, Xtest = get_dummies(Xtrain, Xtest, col_name)\n",
    "\n",
    "# check if the dummies are produced correctly in the trainset\n",
    "Xtrain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with our dummy variables in place, we no longer need old columns\n",
    "def dropfunc(data, column_list):\n",
    "    for column in data:\n",
    "        if column in column_list:\n",
    "            del data[column]\n",
    "\n",
    "# implement the above function on the categorical columns list previously defined\n",
    "dropfunc(Xtrain, catcols)\n",
    "dropfunc(Xtest, catcols)\n",
    "\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc762f",
   "metadata": {},
   "source": [
    "## 5.4 Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "Xtrain_YearBuilt = Xtrain[\"YearBuilt\"].values\n",
    "Xtrain_YearRemodAdd = Xtrain[\"YearRemodAdd\"].values\n",
    "Xtrain_GarageYrBlt = Xtrain[\"GarageYrBlt\"].values\n",
    "Xtrain_YrSold = Xtrain[\"YrSold\"].values\n",
    "Xtrain_MoSold = Xtrain[\"MoSold\"].values\n",
    "Xtrain_new = Xtrain.drop(columns=[\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\", \"YrSold\",\"MoSold\"], axis=1)\n",
    "\n",
    "scaled_vals = scaler.fit_transform(Xtrain_new)\n",
    "Xtrain = pd.DataFrame(scaled_vals, columns=Xtrain_new.columns)\n",
    "\n",
    "# put the non-scaled target back in\n",
    "Xtrain[\"YearBuilt\"] = Xtrain_YearBuilt\n",
    "Xtrain[\"YearRemodAdd\"] = Xtrain_YearRemodAdd\n",
    "Xtrain[\"GarageYrBlt\"] = Xtrain_GarageYrBlt\n",
    "Xtrain[\"YrSold\"] = Xtrain_YrSold\n",
    "Xtrain[\"MoSold\"] = Xtrain_MoSold\n",
    "# inspect the data\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "Xtest_YearBuilt = Xtest[\"YearBuilt\"].values\n",
    "Xtest_YearRemodAdd = Xtest[\"YearRemodAdd\"].values\n",
    "Xtest_GarageYrBlt = Xtest[\"GarageYrBlt\"].values\n",
    "Xtest_YrSold = Xtest[\"YrSold\"].values\n",
    "Xtest_MoSold = Xtest[\"MoSold\"].values\n",
    "Xtest_new = Xtest.drop(columns=[\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\", \"YrSold\",\"MoSold\"], axis=1)\n",
    "\n",
    "scaled_vals = scaler.fit_transform(Xtest_new)\n",
    "Xtest = pd.DataFrame(scaled_vals, columns=Xtest_new.columns)\n",
    "\n",
    "# put the non-scaled target back in\n",
    "Xtest[\"YearBuilt\"] = Xtest_YearBuilt\n",
    "Xtest[\"YearRemodAdd\"] = Xtest_YearRemodAdd\n",
    "Xtest[\"GarageYrBlt\"] = Xtest_GarageYrBlt\n",
    "Xtest[\"YrSold\"] = Xtest_YrSold\n",
    "Xtest[\"MoSold\"] = Xtest_MoSold\n",
    "# inspect the data\n",
    "Xtest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4c35f",
   "metadata": {},
   "source": [
    "# 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d371eaf",
   "metadata": {},
   "source": [
    "1. Random Forest\n",
    "2. Adaboost\n",
    "3. SVR\n",
    "4. Decision Tree\n",
    "5. Catboost\n",
    "6. Elastic Net\n",
    "7. Ridge\n",
    "8. Lightgbm\n",
    "9. Xgboost\n",
    "10. Stacking method and blending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce8395",
   "metadata": {},
   "source": [
    "## 6.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "param_grid = [\n",
    "    {'n_estimators': [30, 50, 100, 150, 200], 'max_depth': [None]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=2022)\n",
    "\n",
    "# we'll use 10-fold cross-validation\n",
    "rf_grid_search = GridSearchCV(forest_reg, param_grid, cv=10, \n",
    "                              scoring='neg_root_mean_squared_error',\n",
    "                              return_train_score=True, verbose=2)\n",
    "\n",
    "rf_grid_search.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0bffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model\n",
    "rf_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf827c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model's RMSE\n",
    "-rf_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bfd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_rmse_score=np.sqrt(-rf_grid_search.best_score_)\n",
    "print(f'The best Random Forest model has a RMSE of: {rf_rmse_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc662c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = rf_grid_search.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef651cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['Predicted'] = yhat\n",
    "sub.to_csv('submission_rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the 20 most important features\n",
    "feature_scores = pd.Series(rf_grid_search.best_estimator_.feature_importances_, index=x_train.columns)\n",
    "feature_scores.nlargest(20).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores.nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acd70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will remove some features that have  very low scores\n",
    "x1_train = Xtrain[['OverallQual','FullBath','GarageCars','TotRmsAbvGrd','LotArea','LotFrontage','TotalBsmtSF',\n",
    "                  'BedroomAbvGr','1stFlrSF','BsmtFinSF1','ScreenPorch','GarageArea','YearBuilt','2ndFlrSF',\n",
    "                  'BsmtFinSF2','MasVnrArea','YearRemodAdd','OverallCond','WoodDeckSF']]\n",
    "x1_test = Xtest[['OverallQual','FullBath','GarageCars','TotRmsAbvGrd','LotArea','LotFrontage','TotalBsmtSF',\n",
    "                  'BedroomAbvGr','1stFlrSF','BsmtFinSF1','ScreenPorch','GarageArea','YearBuilt','2ndFlrSF',\n",
    "                  'BsmtFinSF2','MasVnrArea','YearRemodAdd','OverallCond','WoodDeckSF']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run random forest again\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "param_grid = [\n",
    "    {'n_estimators': [30, 50, 100, 150, 200], 'max_depth': [None]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=2022)\n",
    "\n",
    "# we'll use 10-fold cross-validation\n",
    "rf_grid_search = GridSearchCV(forest_reg, param_grid, cv=10, \n",
    "                              scoring='neg_root_mean_squared_error',\n",
    "                              return_train_score=True, verbose=2)\n",
    "\n",
    "rf_grid_search.fit(x1_train, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9686ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model\n",
    "\n",
    "rf_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model's RMSE\n",
    "\n",
    "-rf_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6442f7b3",
   "metadata": {},
   "source": [
    "The score slightly decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d711d",
   "metadata": {},
   "source": [
    "## 6.2 Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406fa03",
   "metadata": {},
   "source": [
    "Testing between using the original dataset and the dataset that has some columns removed, I found that the new dataset generates a much lower score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_param_grid = [\n",
    " {'C': [1.0, 10, 100,10000],\n",
    " 'gamma': [\"scale\", \"auto\", 0.01, 0.1, 1, 3, 5, 10]\n",
    " },\n",
    "]\n",
    "svr = SVR(kernel=\"rbf\")\n",
    "\n",
    "#10 fold cross_validation and access to train score for later\n",
    "sv_grid_search = GridSearchCV(svr, svr_param_grid, cv=10, scoring='neg_root_mean_squared_error',\n",
    " return_train_score=True, verbose=2)\n",
    "# fit the best model and hyperparameters to the training set\n",
    "sv_grid_search.fit(x1_train, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best svr model\n",
    "best_sv = sv_grid_search.best_estimator_\n",
    "best_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516331ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model's RMSE\n",
    "-sv_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_yhat = best_sv.predict(x1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['Predicted'] = svr_yhat\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9cf3e",
   "metadata": {},
   "source": [
    "## 6.3 Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ed590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adaboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "adb_reg=AdaBoostRegressor()\n",
    "adb_param_grid = {'n_estimators': [3, 10, 20, 50], 'learning_rate': [0.001, 0.01, 0.1, 0.25, 0.5, 0.75,\n",
    "1],\n",
    " 'loss' : ['linear', 'square', 'exponential']}\n",
    "adb_reg=AdaBoostRegressor(random_state=2022)\n",
    "\n",
    "# we'll use 10-fold cross-validation and want to have access to the train score\n",
    "adb_random_grid_search = RandomizedSearchCV(adb_reg, adb_param_grid, cv=10, n_iter=10,\n",
    " scoring='neg_root_mean_squared_error', random_state=2022, return_train_score=True)\n",
    "#fit the best model and hyperparameters to the training dataset\n",
    "adb_random_grid_search.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e634a99",
   "metadata": {},
   "outputs": [],
   "source": [
    " # the best model\n",
    "best_adb = adb_random_grid_search.best_estimator_\n",
    "best_adb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model's RMSE\n",
    "-adb_random_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe18f8",
   "metadata": {},
   "source": [
    "## 6.4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df94f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#We will search for the best hyperparameters for the decision trees, using GridSearch\n",
    "# and thus cross-validation. We give here several combinations for the hyperparameters to compare.\n",
    "dt_param_grid= {'min_samples_split': [2, 3, 4, 5], 'max_depth': [2, 4, 6, 8, None]}\n",
    "\n",
    "#n_estimators: Number of trees in random forest\n",
    "#max_depth: Maximum number of levels in tree\n",
    "# min_samples_split: Minimum number of samples required to split a node\n",
    "Dec_tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "#Cross-validation with 10 splits\n",
    "# we also want it to return the train score later\n",
    "dt_grid_search = GridSearchCV(Dec_tree_reg, dt_param_grid, cv=10,\n",
    " scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "#We fit the training data to the best model (and thus estimators)\n",
    "dt_grid_search.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# details on the best model for the decision tree algorithm\n",
    "best_dt=dt_grid_search.best_estimator_\n",
    "best_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac14e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model's RMSE\n",
    "-dt_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d57fa6",
   "metadata": {},
   "source": [
    "## 6.5 Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe07b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model = CatBoostRegressor()\n",
    "parameters = {'depth' : [6,8,10],\n",
    "              'learning_rate' : [0.02,0.05,0.1],\n",
    "              'iterations'    : [70,80,500],\n",
    "              }\n",
    "\n",
    "catboost = TuneGridSearchCV(estimator=model, scoring=\"neg_root_mean_squared_error\", param_grid = parameters, cv = 10, n_jobs=-1,refit=True)\n",
    "catboost.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff80bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the best model \n",
    "best_cb = catboost.best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model's RMSE\n",
    "-catboost.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the important features for catboost\n",
    "feature_scores = pd.Series(best_cb.feature_importances_, index=Xtrain.columns)\n",
    "feature_scores.nlargest(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ac148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "cat_yhat = catboost.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['Predicted'] = cat_yhat\n",
    "sub.to_csv('submission_catboost.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17516a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test catboost when removing an unimportant feature\n",
    "x2_train =Xtrain.drop(columns=['MasVnrArea'])\n",
    "x2_test =Xtest.drop(columns=['MasVnrArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0dc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from catboost import CatBoostRegressor\n",
    "model = CatBoostRegressor()\n",
    "parameters = {'depth' : [6,8,10],\n",
    "              'learning_rate' : [0.02,0.05,0.1],\n",
    "              'iterations'    : [70,80,500]\n",
    "              }\n",
    "\n",
    "grid = TuneGridSearchCV(estimator=model, scoring=\"neg_root_mean_squared_error\", param_grid = parameters, cv = 10, n_jobs=-1)\n",
    "grid.fit(x2_train, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33fae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the best score\n",
    "-grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cb = grid.best_estimator_\n",
    "best_cb \n",
    "cat1_yhat = best_cb.predict(x2_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98968aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['Predicted'] = cat_yhat\n",
    "sub.to_csv('submission_catboost2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ac3e8",
   "metadata": {},
   "source": [
    "## 6.6 Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elastic net\n",
    "# evaluate an elastic net model on the dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from tune_sklearn import TuneGridSearchCV\n",
    "\n",
    "eNet = ElasticNet()\n",
    "\n",
    "parametersGrid = {\"max_iter\": [1, 5, 10],\n",
    "                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n",
    "\n",
    "elastic = TuneGridSearchCV(estimator=eNet, scoring=\"neg_root_mean_squared_error\", param_grid = parametersGrid, cv = 10, n_jobs=-1,refit=True)\n",
    "elastic.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the best score\n",
    "-elastic.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = elastic.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a958e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['Predicted'] = ypred\n",
    "sub.to_csv('submission_Enet.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0071ff3",
   "metadata": {},
   "source": [
    "## 6.7 Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "model_ridge = Ridge()\n",
    "parameters = {'alpha':[0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]}\n",
    "\n",
    "\n",
    "\n",
    "model_ridge= TuneGridSearchCV(estimator=model_ridge, scoring=\"neg_root_mean_squared_error\", param_grid = parameters, cv = 10, n_jobs=-1,refit=True).fit(Xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349dabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the best score\n",
    "-model_ridge.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction \n",
    "model_ridge_pred = model_ridge.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['Predicted'] = model_ridge_pred\n",
    "sub.to_csv('submission_ridge.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9758de",
   "metadata": {},
   "source": [
    "# 6.8. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d198c7",
   "metadata": {},
   "source": [
    "Since xgboost takes a lot of time to run, I'll test the value of the parameters beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing grid search\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBRegressor(objective ='reg:linear',tree_method = \"hist\")\n",
    "XGBRegressor_search = GridSearchCV(model, {'min_child_weight': [1, 5, 10]}, cv=10, scoring=\"neg_root_mean_squared_error\")\n",
    "XGBRegressor_search.fit(Xtrain, ytrain)\n",
    "XGBRegressor_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02937911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBRegressor(objective ='reg:linear',tree_method = \"hist\", random_state=2022)\n",
    "parameters = {'max_depth': [5],\n",
    "              'gamma': [0.5],\n",
    "              'colsample_bytree': [1.0],\n",
    "              'subsample': [1.0],\n",
    "              'min_child_weight': [5]\n",
    "              }\n",
    "\n",
    "xgboost = TuneGridSearchCV(estimator=model,scoring=\"neg_root_mean_squared_error\", param_grid = parameters, cv = 10, n_jobs=-1, refit=True)\n",
    "xgboost.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b939679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best score\n",
    "-xgboost.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b43311",
   "metadata": {},
   "source": [
    "## 6.9 Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d64a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing grid search\n",
    "import scipy as scipy\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier, Booster\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "model = lightgbm.LGBMRegressor()\n",
    "lightgbm_search = GridSearchCV(model, {'num_leaves': [6,50,100]}, cv=10, scoring=\"neg_root_mean_squared_error\")\n",
    "lightgbm_search.fit(Xtrain, ytrain)\n",
    "lightgbm_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier, Booster\n",
    "import lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "#light GBM\n",
    "\n",
    "model = lightgbm.LGBMRegressor()\n",
    "parameters = {'num_leaves': [50],\n",
    "    'reg_alpha': [5],\n",
    "    'min_data_in_leaf': [30],\n",
    "    'lambda_l1': [0.7000000000000001],\n",
    "    'lambda_l2':[0.2],\n",
    "    'reg_lambda': [0],\n",
    "    'min_child_weight': [1e-5],\n",
    "    'boosting':['gbdt'],\n",
    "    'learning_rate':[0.02],\n",
    "    'drop_rate':[0.1],\n",
    "    'subsample':[0.3],\n",
    "    'extra_trees':[True],\n",
    "    'skip_drop':[0],\n",
    "            \n",
    "        \n",
    "        \n",
    "              }\n",
    "\n",
    "\n",
    "\n",
    "lightgbm = TuneGridSearchCV(estimator=model, scoring=\"neg_root_mean_squared_error\", param_grid = parameters, cv = 10, n_jobs=-1,refit=True)\n",
    "lightgbm.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f69f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best score\n",
    "-lightgbm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ad446",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_yhat =lightgbm.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8999c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['Predicted'] = lgbm_yhat\n",
    "sub.to_csv('submission_lgbm.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32046b52",
   "metadata": {},
   "source": [
    "## 6.10 Stack Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "stack_gen = TuneGridSearchCV(StackingCVRegressor(regressors=(model_ridge, catboost,xgboost, lightgbm,forest_reg),\n",
    "                                meta_regressor=catboost,\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162897f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_gen.fit(np.array(Xtrain), np.array(ytrain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmse_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=2022).get_n_splits(Xtrain.values)\n",
    "    rmse= -cross_val_score(stack_gen, Xtrain.values, ytrain, scoring=\"neg_root_mean_squared_error\", cv = kf)\n",
    "    return(rmse)\n",
    "rmse_cv(stack_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f29c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "y_pred_stack = stack_gen.predict(np.array(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6168e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_ID\n",
    "sub['Predicted'] = y_pred_stack\n",
    "sub.to_csv('submission_stack.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing and submitting the result\n",
    "sub1 = pd.read_csv(\"submission_ridge.csv\")\n",
    "sub2 = pd.read_csv(\"submission_catboost.csv\")\n",
    "sub3 = pd.read_csv(\"submission_stack.csv\")\n",
    "sub4 = pd.read_csv(\"blend_submission2.csv\")\n",
    "blend= pd.read_csv(\"submission.csv\")\n",
    "blend['Predicted'] = sub1['Predicted']*0.3 + sub2['Predicted']*0.7\n",
    "blend.to_csv('blend_submission1.csv', index=False )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
